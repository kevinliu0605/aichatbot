Oftentimes a dataset can be partitioned into different categories. A doctor may notice that their patients come in cohorts and different cohorts respond to different treatments. A biologist may gain insight by identifying that bats and whales, despite outward appearances, have some underlying similarity, and both should be considered members of the same category, i.e., “mammal”. The problem of automatically identifying meaningful groupings in datasets is called clustering. Once these groupings are found, they can be leveraged toward interpreting the data and making optimal decisions for each group.
Mathematically, clustering looks a bit like classification: we wish to find a mapping from datapoints, x, to categories, y. However, rather than the categories being predefined labels, the categories in clustering are automatically discovered partitions of an unlabeled dataset.
Because clustering does not learn from labeled examples, it is an example of an unsupervised learning algorithm. Instead of mimicking the mapping implicit in supervised training pairs {x^(i), y^(i)} , clustering assigns datapoints to categories based on how the unlabeled data {x^(i)} is distributed in data space.
Intuitively, a “cluster” is a group of datapoints that are all nearby to each other and far away from other clusters. 
There seem to be about five clumps of datapoints and those clumps are what we would like to call clusters. If we assign all datapoints in each clump to a cluster corresponding to that clump, then we might desire that nearby datapoints are assigned to the same cluster, while far apart datapoints are assigned to different clusters.
In designing clustering algorithms, three critical things we need to decide are: 1. How do we measure distance between datapoints? What counts as “nearby” and “far apart”? 2. How many clusters should we look for? 3. How do we evaluate how good a clustering is?
One of the simplest and most commonly used clustering algorithms is called k-means. The goal of the k-means algorithm is to assign datapoints to k clusters in such a way that the variance within clusters is as small as possible. Notice that this matches our intuitive idea that a cluster should be a tightly packed set of datapoints. Similar to the way we showed that supervised learning could be formalized mathematically as the minimization of an objective function (loss function + regularization), we will show how unsupervised learning can also be formalized as minimizing an objective function. Let us denote the cluster assignment for a datapoint x^(i) as y^(i) ∈ {1, 2, . . . , k}, i.e., y^(i) = 1 means we are assigning datapoint x^(i) to cluster number 1. Then the k-means objective can be quantified with the following objective function (which we also call the “k-means objective” or “k-means loss”): Sum_{j=1}^{k} Sum_{i=1}^{n} 1(y^(i) = j) ||x^(i) - μ_(j)||^2, where μ^(j) = (1 / N_j) * Σ_{i=1}^n 1(y^(i) = j) * x^(i) and N_j = Σ_{i=1}^n 1(y^(i) = j), so that μ^(j) is the mean of all datapoints in cluster j, and using 1(·) to denote the indicator function (which takes on value of 1 if its argument is true and 0 otherwise). The inner sum (over data points) of the loss is the variance of datapoints within cluster j. We sum up the variance of all k clusters to get our overall loss.
The k-means algorithm minimizes this loss by alternating between two steps: given some initial cluster assignments: 1) compute the mean of all data in each cluster and assign this as the “cluster mean”, and 2) reassign each datapoint to the cluster with nearest cluster mean. Each time we reassign the data to the nearest cluster mean, the k-means loss decreases (the datapoints end up closer to their assigned cluster mean), or stays the same. And each time we recompute the cluster means the loss also decreases (the means end up closer to their assigned datapoints) or stays the same. Overall then, the clustering gets better and better, according to our objective – until it stops improving. After four iterations of cluster assignment + update means in our example, the k-means algorithm stops improving.
The for-loop over the n datapoints assigns each datapoint to the nearest cluster center. The for-loop over the k clusters updates the cluster center to be the mean of all datapoints currently assigned to that cluster. As suggested above, it can be shown that this algorithm reduces the loss in Eq. 12.1 on each iteration, until it converges to a local minimum of the loss.
It’s like classification except the algorithm picked what the classes are rather than being given examples of what the classes are.
We can also use gradient descent to optimize the k-means objective. To show how to apply gradient descent, we first rewrite the objective as a differentiable function only of µ: L(μ) = Σ_{i=1}^n min_j ||x^(i) - μ_(j)||^2
L(µ) is the value of the k-means loss given that we pick the optimal assignments of the datapoints to cluster means (that’s what the minj does). Now we can use the gradient (∂L(µ))/(∂µ) to find the values for µ that achieve minimum loss when cluster assignments are optimal. Finally, we read off the optimal cluster assignments, given the optimized µ, just by assigning datapoints to their nearest cluster mean: y^(i) = argmin_j ||x^(i) - μ_(j)||^2
This procedure yields a local minimum of Eq. 12.1, as does the standard k-means algorithm we presented (though they might arrive at different solutions). It might not be a global optimum since the objective is not convex (due to minj , as the minimum of multiple convex functions is not necessarily convex).
The standard k-means algorithm, as well as the variant that uses gradient descent, both are only guaranteed to converge to a local minimum, not necessarily a global minimum of the loss. Thus the answer we get out depends on how we initialize the cluster means.
A variety of methods have been developed to pick good initializations (for example, check out the k-means++ algorithm). One simple option is to run the standard k-means algorithm multiple times, with different random initial conditions, and then pick from these the clustering that achieves the lowest k-means loss.
A very important choice in cluster algorithms is the number of clusters we are looking for. Some advanced algorithms can automatically infer a suitable number of clusters, but most of the time, like with k-means, we will have to pick k – it’s a hyperparameter of the algorithm.
Alternatively, you may be wondering: why bother picking a single k? Wouldn’t it be nice to reveal a hierarchy of clusterings of our data, showing both coarse and fine groupings? Indeed hierarchical clustering is another important class of clustering algorithms, beyond k-means. These methods can be useful for discovering tree-like structure in data, andthey work a bit like this: initially a coarse split/clustering of the data is applied at the root of the tree, and then as we descend the tree we split and cluster the data in ever more finegrained ways. A prototypical example of hierarchical clustering is to discover a taxonomy of life, where creatures may be grouped at multiple granularities, from species to families to kingdoms.
Clustering algorithms group data based on a notion of similarity, and thus we need to define a distance metric between datapoints. This notion will also be useful in other machine learning approaches, such as nearest-neighbor methods that we see in Chapter 9. In kmeans and other methods, our choice of distance metric can have a big impact on the results we will find.
Our k-means algorithm uses the Euclidean distance, i.e., x(i) − µ(j) , with a loss function that is the square of this distance. We can modify k-means to use different distance metrics, but a more common trick is to stick with Euclidean distance but measured in a feature space. Just like we did for regression and classification problems, we can define a feature map from the data to a nicer feature representation, ϕ(x), and then apply k-means to cluster the data in the feature space.
As a simple example, suppose we have two-dimensional data that is very stretched out in the first dimension and has less dynamic range in the second dimension. Then we may want to scale the dimensions so that each has similar dynamic range, prior to clustering. We could use standardization, like we did in Chapter 5.
If we want to cluster more complex data, like images, music, chemical compounds, etc., then we will usually need more sophisticated feature representations. One common practice these days is to use feature representations learned with a neural network. For example, we can use an autoencoder to compress images into feature vectors, then cluster those feature vectors.
One of the hardest aspects of clustering is knowing how to evaluate it. This is actually a big issue for all unsupervised learning methods, since we are just looking for patterns in the data, rather than explicitly trying to predict target values (which was the case with supervised learning).
Remember, evaluation metrics are not the same as loss functions, so we can’t just measure success by looking at the k-means loss. In prediction problems, it is critical that the evaluation is on a held-out test set, while the loss is computed over training data. If we evaluate on training data we cannot detect overfitting. Something similar is going on with the example in Section 12.1.6, where setting k to be too large can precisely “fit” the data (minimize the loss), but yields no general insight.
One way to evaluate our clusters is to look at the consistency with which they are found when we run on different subsamples of our training data, or with different hyperparameters of our clustering algorithm (e.g., initializations). For example, if running on several bootstrapped samples (random subsets of our data) results in very different clusters, it should call into question the validity of any of the individual results.
If we have some notion of what ground truth clusters should be, e.g., a few data points that we know should be in the same cluster, then we can measure whether or not our discovered clusters group these examples correctly.
Clustering is often used for visualization and interpretability, to make it easier for humans to understand the data. Here, human judgment may guide the choice of clustering algorithm. More quantitatively, discovered clusters may be used as input to downstream tasks. For example, we may fit a different regression function on the data within each cluster. Figure 12.6 gives an example where this might be useful. In cases like this, the success of a clustering algorithm can be indirectly measured based on the success of the downstream application (e.g., does it make the downstream predictions more accurate).
Assume that we have input data 𝐃 = {𝒙(1), ..., 𝒙(𝑛)}, where 𝒙(𝑖) ∈ ℝ^𝑑. We seek to learn an autoencoder that will output a new dataset 𝐃_out = {𝒂(1), ..., 𝒂(𝑛)}, where 𝒂(𝑖) ∈ ℝ^𝑘 with 𝑘 < 𝑑. We can think about 𝒂(𝑖) as the new representation of data point 𝒙(𝑖). For example, in Fig. 12.7 we show the learned representations of a dataset of MNIST digits with 𝑘 = 2. We see, after inspecting the individual data points, that unsupervised learning has found a compressed (or latent) representation where images of the same digit are close to each other, potentially greatly aiding subsequent clustering or classification tasks.
Formally, an autoencoder consists of two functions, a vector-valued encoder 𝑔 : ℝ^𝑑 → ℝ^𝑘 that deterministically maps the data to the representation space 𝒂 ∈ ℝ^𝑘, and a decoder ℎ : ℝ^𝑘 → ℝ^𝑑 that maps the representation space back into the original data space.
In general, the encoder and decoder functions might be any functions appropriate to the domain. Here, we are particularly interested in neural network embodiments of encoders and decoders. The basic architecture of one such autoencoder, consisting of only a single layer neural network in each of the encoder and decoder, is shown in Figure 12.8; note that bias terms 𝑾₀¹ and 𝑾₀² into the summation nodes exist, but are omitted for clarity in the figure. In this example, the original 𝑑-dimensional input is compressed into 𝑘 = 3 dimensions via the encoder 𝑔(𝒙; 𝑾₀¹, 𝑾¹) = 𝑓₁(𝑾¹ᵀ𝒙 + 𝑾₀¹) with 𝑾¹ ∈ ℝ^(𝑑×𝑘) and 𝑾₀¹ ∈ ℝ^𝑘, and where the non-linearity 𝑓₁ is applied to each dimension of the vector. To recover (an approximation to) the original instance, we then apply the decoder ℎ(𝒂; 𝑾₀², 𝑾²) = 𝑓₂(𝑾²ᵀ𝒂 + 𝑾₀²), where 𝑓₂ denotes a different non-linearity (activation function). In general, both the decoder and the encoder could involve multiple layers, as opposed to the single layer shown here. Learning seeks parameters W¹, W₀¹ and W², W₀² such that the reconstructed instances, h(g(x^(i); W¹, W₀¹); W², W₀²), are close to the original input x^(i).
We learn the weights in an autoencoder using the same tools that we previously used for supervised learning, namely (stochastic) gradient descent of a multi-layer neural network to minimize a loss function. All that remains is to specify the loss function 𝓛(𝑥̃, 𝑥), which tells us how to measure the discrepancy between the reconstruction 𝑥̃ = h(g(𝑥; W¹, W₀¹); W², W₀²) and the original input 𝑥.
and the original input 𝑥. For example, for continuous-valued 𝑥 it might make sense to use squared loss, i.e., 𝓛_SE(𝑥̃, 𝑥) = Σ_{j=1}^d (𝑥_j − 𝑥̃_j)². Learning then seeks to optimize the parameters of ℎ and 𝑔 so as to minimize the reconstruction error, measured according to this loss function: min_{W¹, W₀¹, W², W₀²} Σ_{i=1}^n 𝓛_SE(h(g(𝑥^(i); W¹, W₀¹); W², W₀²), 𝑥^(i))
What makes a good learned representation in an autoencoder? Notice that, without further constraints, it is always possible to perfectly reconstruct the input. For example, we could let k = d and h and g be the identity functions. In this case, we would not obtain any compression of the data.
To learn something useful, we must create a bottleneck by making k to be smaller (often much smaller) than d. This forces the learning algorithm to seek transformations that describe the original data using as simple a description as possible. Thinking back to the digits dataset, for example, an example of a compressed representation might be the digit label (i.e., 0–9), rotation, and stroke thickness. Of course, there is no guarantee that the learning algorithm will discover precisely this representation. After learning, we can inspect the learned representations, such as by artificially increasing or decreasing one of the dimensions (e.g., a₁) and seeing how it affects the output h(a), to try to better understand what it has learned.
As with clustering, autoencoders can be a preliminary step toward building other models, such as a regressor or classifier. For example, once a good encoder has been learned, the decoder might be replaced with another neural network that is then trained with supervised learning (perhaps using a smaller dataset that does include labels).
We close by mentioning that even linear encoders and decoders can be very powerful. In this case, rather than minimizing the above objective with gradient descent, a technique called principal components analysis (PCA) can be used to obtain a closed-form solution to the optimization problem using a singular value decomposition (SVD). Just as a multilayer neural network with nonlinear activations for regression (learned by gradient descent) can be thought of as a nonlinear generalization of a linear regressor (fit by matrix algebraic operations), the neural network-based autoencoders discussed above (and learned with gradient descent) can be thought of as a generalization of linear PCA (as solved with matrix algebra by SVD).
Advanced neural networks that build on the encoder-decoder conceptual decomposition have become increasingly powerful in recent years. One family of applications are generative networks, where new outputs that are "similar to" but different from any existing training sample are desired. In variational autoencoders, the compressed representation encompasses information about the probability distribution of training samples, e.g., learning both mean and standard deviation variables in the bottleneck layer or latent representation. Then, new outputs can be generated by random sampling based on the latent representation variables and feeding those samples into the decoder. For instance, Transformers use multiple encoder and decoder blocks, together with a self-attention mechanism to make predictions about potential next outputs resulting from sequences of inputs. Such transformer networks have many applications in natural language processing and elsewhere.